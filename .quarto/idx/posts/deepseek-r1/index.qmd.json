{"title":"Explain DeepSeek-R1-Zero to My Little Aurora","markdown":{"yaml":{"title":"Explain DeepSeek-R1-Zero to My Little Aurora","author":"Zhong Zhang","date":"2025-09-14","categories":["AI","LLM","RL"],"description":"DeepSeek-R1-Zero plain explanation"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n“There are two major types of learning, in both children and in deep learning. There is 1) imitation learning (watch and repeat, i.e. pretraining, supervised finetuning), and 2) trial-and-error learning (reinforcement learning). Almost every single shocking result of deep learning, and the source of all *magic* is always 2. 2 is the \"aha moment\" when DeepSeek (or o1 etc.) discovers that it works well to re-evaluate your assumptions, backtrack, try something else, etc.”  quote from Andrej Karpathy.\n\nWe answer four questions about DeepSeek-R1-Zero\nWhat are the contributions of DeepSeek-R1-Zero? \nWhat does the DeepSeek-R1-Zero training process look like? \nunderstand the Group Relative Policy Optimization (GRPO) algorithm in detail \nWhat makes R1-zero work, is it GRPO or something else?\n\nDon’t worry about these Reinforcement Learning (RL) and Large Language Model (LLM) concepts, e.g. policy, action, etc. They don’t affect the understanding, which is the purpose of this article. I translate these concepts to its corresponding terms in the context of DeepSeek-R1-Zero in the below table. If there is anything unclear, please comment. So, let’s get started!\n\n| RL/LLM Concepts | Corresponding terms in the context |\n|-----------------|-------------------------------------|\n| Policy | LLM model |\n| Policy Action | The output of the LLM given a prompt |\n| Prompt | Training example |\n\n## Question 1: What are the contributions of DeepSeek-R1-Zero? \n\nDeepSeek-R1-Zero's key breakthrough is proving that large language models (LLM) can develop strong reasoning abilities a.k.a problem solving skills through practice alone, without first being shown how to solve problems. Think about learning mathematics: traditionally, a teacher first demonstrates how to solve each type of problem step-by-step, and then students practice similar problems on their own. DeepSeek-R1-Zero shows that an AI can become excellent at problem-solving by skipping the demonstration phase and jumping straight to practice – learning purely from trial and error. This not only simplifies the training process but also allows the AI to discover its own creative ways of reasoning, rather than just imitating human examples.\n\nLet’s think of training an AI model like teaching a student. Credits go to Andrej Karpathy\n\n- **Building a Foundation (Pretraining):** First, the child learns basic knowledge - reading, writing, math facts. This is like “pretraining” an AI, where it absorbs vast amounts of information from the internet, building a foundation of general knowledge. \n\n- **Showing Examples (Supervised Fine-Tuning/SFT):** is like showing the student solved examples with detailed steps. A teacher explains step-by-step how to solve specific problems, similar to how models are fine-tuned on ideal step-by-step responses written by humans.\n\n- **Practice Makes Perfect (Reinforcement Learning/RL):**  Finally, the child practices solving problems on their own, getting feedback on whether they got the right answer (but not how to get there). They learn through trial and error, figuring out their own strategies. This is like \"Reinforcement Learning\" (RL), where the AI is given problems and rewarded for finding the correct solution, even if it takes a roundabout path.\n\n<div style=\"text-align: center;\">\n![pretraining, SFT, RL](pretraining_sft_rl.jpeg)\n</div>\n\nDeepSeek-R1-Zero demonstrates that large language models (LLM) can develop strong reasoning skills using only Reinforcement Learning (RL), without needing the traditional step of Supervised Fine-Tuning (SFT). Unlike previous methods that relied heavily on labeled data (showing the AI how to solve problems step by step) to teach models how to reason, DeepSeek-R1-Zero shows that reasoning abilities—like checking its own answers, thinking step-by-step, etc—can emerge naturally through RL alone. This breakthrough proves that models can learn to solve complex problems just by practicing and receiving feedback (correct or not), simplifying the training process and opening potentially more powerful ways to build AI reasoning capabilities.\n\n<div style=\"text-align: center;\">\n![Pure RL VS SFT](RL_vs_SFT.jpg)\n</div>\n\n\nNote: this practice problem will go through this article as an example until the end. \n\n## Question 2: What does the DeepSeek-R1-Zero training process look like?\n\nThe goal of training is to \"steer\" the base LLM (like GPT-4o or DeepSeek V3) towards generating correct and well-formatted answers more consistently. Think of it like upgrading the model – from GPT-4o to a hypothetical \"o3-mini-high,\" or from DeepSeek V3 to DeepSeek-R1-Zero. \n\nThe left side of the below figure is the base model while the right side is the prompted version with GRPO based RL training. As you can see the left one fails at the question while the right one succeeds also with the reasoning process. The figure comes from reference 4.  \n\n<div style=\"text-align: center;\">\n![With VS Without Reasoning](with_vs_without_reasoning.jpg)\n</div>\n\nHere's how it works:\n\nPrompting the Model: We feed a practice problem (like the math problem about Natalia's clips in the below table) to the current version of the LLM, which we'll call the \"old LLM.\"\n\nGenerating Responses: run the old LLM with the practice problem as prompt several times to get answers (technically called \"completions\"). Some of these might be correct, some incorrect, and some might have formatting issues.\n\nScoring the Responses: A \"reward function\" evaluates each response, assigning scores based on:\nCorrectness: How accurate the answer is.\nFormat: Whether the response follows a desired structure (e.g., showing its steps).\n\nUpdating the Model: The crucial step! Using a specialized algorithm called GRPO (Generalized Relative Policy Optimization), the LLM's internal parameters (the \"weights\" that control its behavior) are adjusted. These adjustments are guided by the reward scores: responses with higher scores \"pull\" the model's parameters in their direction, making it more likely to generate similar responses in the future. This updated model is now the \"new LLM.\"\n\nIteration: We set the \"old LLM\" to be the \"new LLM\" (the updated version) and repeat steps 1-4 many, many times. With each iteration, the model gets progressively better at generating correct and well-formatted answers. This iterative process continues until we have a version of the LLM with significantly improved reasoning abilities – in this case, DeepSeek-R1-Zero.\n\nIt's like teaching through trial and error - the model keeps attempting problems, learns from its successes and failures, and gradually becomes better at solving them. The final result is a more capable model like DeepSeek-R1-Zero.\n\nLet’s dive deep into reward functions a bit. The DeepSeek paper mentioned two types of rewards: accuracy and format. An implementation could be but not limited to as below. \n\n\n## Code Example\n\n```python\n# Your code here\ndef correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n    responses = [completion[0]['content'] for completion in completions]\n    q = prompts[0][-1]['content']\n    extracted_responses = [extract_xml_answer(r) for r in responses]\n    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n```\n\n```python\n# Your code here\ndef strict_format_reward_func(completions, **kwargs) -> list[float]:\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.match(pattern, r) for r in responses] \n    return [0.5 if match else 0.0 for match in matches]\n```\n\n## Question 3: understand the Group Relative Policy Optimization (GRPO) algorithm in detail\n\n<div style=\"text-align: center;\">\n![](grpo1.jpg)\n![](grpo2.jpg)\n</div>\n\nThe GRPO algorithm, as described in the paper, begins by sampling a group of outputs {o_1, o_2, ..., o_G} from the old LLM (denoted as π_θold). This is done by feeding the same training example (the prompt, q) to the old LLM multiple times and collecting the different responses it generates.\n\nThe training then focuses on maximizing the objective function (equation (1)). This objective function is, at its core, a complex function of the new LLM's parameters (θ). Standard gradient descent algorithms can be used to efficiently update these parameters and maximize the function's value.\n\nThe key to understanding how GRPO improves the LLM lies in maximizing the objective function (equation (1)). And the heart of that process is within the part highlighted by the \"red rectangle.\"\n\nLet’s zoom into the red rectangle, which contains two parts: (1) probability ratio (2) Advantage function Ai.\n**Advantage function** Ai is defined in the equation (3) and pretty straightforward. A_i tells us whether a particular output (o_i) is better or worse than the average output for a given prompt.\nIf the reward (R_i) for o_i is higher than the average reward, A_i is positive (good).\nIf the reward for o_i is lower than the average reward, A_i is negative (bad).\n \n**Probability ratio:** measures how much the probability of generating the completion o_i for prompt q has changed between the new policy π_\\theta and the old policy π_{\\theta_{old}}.\n- Ratio > 1: The new LLM is more likely to generate o_i than the old LLM.\n- Ratio < 1: The new LLM is less likely to generate o_i than the old LLM.\n\n**The Magic: Multiplying Ratio and Advantage**\n- If A_i > 0 (good completion) and Ratio > 1 (new policy makes it more likely), we want to encourage this change. We want to increase the probability of generating this good completion even more.\n- If A_i > 0 and Ratio < 1 (new policy makes it less likely), we want to discourage this change. We want to increase the probability of generating this good completion.\n- If A_i < 0 (bad completion) and Ratio > 1 (new policy makes it more likely), we want to discourage this change. We want to decrease the probability of generating this bad completion.\n- If A_i < 0 and Ratio < 1 (new policy makes it less likely), we want to encourage this change. We want to decrease the probability of generating this bad completion even further.\n\nBeyond the Red Rectangle: Stability and Generalization\n\nThe \"clip\" and \"KL\" terms in the full objective function play important roles in ensuring the training process remains stable and the model generalizes well:\n\n- Clipping (clip): The clip function acts like a \"brake,\" preventing the probability ratio from changing too drastically in a single update. This avoids large, potentially destabilizing parameter adjustments.\n\n- KL Divergence (KL): The KL divergence term measures how much the new LLM's behavior differs from a \"reference\" policy (often the old LLM). By penalizing large deviations, we encourage the model to improve incrementally and retain some of its desirable characteristics. This helps the model perform well on new, unseen problems and encourages exploration of different solutions.\n\n## Question 4: What makes R1-zero work, is it GRPO or something else?\n\nAs an engineer with no prior LLM/RL experience, I found the GRPO algorithm surprisingly understandable after just a weekend of study. It's essentially a refined version of the established PPO algorithm, so for experts in the field, it's likely not a revolutionary leap. The real challenge, and where DeepSeek-R1-Zero truly shines, is in the engineering implementation, for example how to define the reward functions. This highlights the critical importance of exceptionally talented engineers, perhaps even more so than simply acquiring more computing power.\n\nThe idea of using pure RL to improve a base model isn't new, but previous attempts haven't matched DeepSeek's results. I suspect two key factors are at play: first, a very strong foundation in the base model itself; and second, the quality of the training data (the practice problems). DeepSeek hasn't open-sourced their training data, which is a strong indication of its crucial and proprietary nature.\n\nIn essence, DeepSeek-R1-Zero's success likely boils down to a potent combination of: brilliant engineering, carefully curated training examples, and a robust base model.\n\nThis article is a collaboration with DeepSeek-R1, Gemini 2.0 Flash Thinking Experimental 01-21 (Google really needs to work hard on the naming), ChatGPT o3-mini-high, Claude. Gemini is slightly better than R1, but both of them are better than o3-mini-high in terms of reasoning. Claude is still really good at polishing writing but Gemini 2.0 Pro Experimental 02-05 is at least comparable. The conclusion is there is no need to be a member of Claude anymore. :-). \n\n## Reference\n- 深度强化学习(1/5)：基本概念  Deep Reinforcement Learning (1/5)\n- https://x.com/karpathy/status/1885026028428681698\n- DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, link\n- Train your own R1 reasoning model with Unsloth (GRPO) link\n- https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb","srcMarkdownNoYaml":"\n\n## Introduction\n\n“There are two major types of learning, in both children and in deep learning. There is 1) imitation learning (watch and repeat, i.e. pretraining, supervised finetuning), and 2) trial-and-error learning (reinforcement learning). Almost every single shocking result of deep learning, and the source of all *magic* is always 2. 2 is the \"aha moment\" when DeepSeek (or o1 etc.) discovers that it works well to re-evaluate your assumptions, backtrack, try something else, etc.”  quote from Andrej Karpathy.\n\nWe answer four questions about DeepSeek-R1-Zero\nWhat are the contributions of DeepSeek-R1-Zero? \nWhat does the DeepSeek-R1-Zero training process look like? \nunderstand the Group Relative Policy Optimization (GRPO) algorithm in detail \nWhat makes R1-zero work, is it GRPO or something else?\n\nDon’t worry about these Reinforcement Learning (RL) and Large Language Model (LLM) concepts, e.g. policy, action, etc. They don’t affect the understanding, which is the purpose of this article. I translate these concepts to its corresponding terms in the context of DeepSeek-R1-Zero in the below table. If there is anything unclear, please comment. So, let’s get started!\n\n| RL/LLM Concepts | Corresponding terms in the context |\n|-----------------|-------------------------------------|\n| Policy | LLM model |\n| Policy Action | The output of the LLM given a prompt |\n| Prompt | Training example |\n\n## Question 1: What are the contributions of DeepSeek-R1-Zero? \n\nDeepSeek-R1-Zero's key breakthrough is proving that large language models (LLM) can develop strong reasoning abilities a.k.a problem solving skills through practice alone, without first being shown how to solve problems. Think about learning mathematics: traditionally, a teacher first demonstrates how to solve each type of problem step-by-step, and then students practice similar problems on their own. DeepSeek-R1-Zero shows that an AI can become excellent at problem-solving by skipping the demonstration phase and jumping straight to practice – learning purely from trial and error. This not only simplifies the training process but also allows the AI to discover its own creative ways of reasoning, rather than just imitating human examples.\n\nLet’s think of training an AI model like teaching a student. Credits go to Andrej Karpathy\n\n- **Building a Foundation (Pretraining):** First, the child learns basic knowledge - reading, writing, math facts. This is like “pretraining” an AI, where it absorbs vast amounts of information from the internet, building a foundation of general knowledge. \n\n- **Showing Examples (Supervised Fine-Tuning/SFT):** is like showing the student solved examples with detailed steps. A teacher explains step-by-step how to solve specific problems, similar to how models are fine-tuned on ideal step-by-step responses written by humans.\n\n- **Practice Makes Perfect (Reinforcement Learning/RL):**  Finally, the child practices solving problems on their own, getting feedback on whether they got the right answer (but not how to get there). They learn through trial and error, figuring out their own strategies. This is like \"Reinforcement Learning\" (RL), where the AI is given problems and rewarded for finding the correct solution, even if it takes a roundabout path.\n\n<div style=\"text-align: center;\">\n![pretraining, SFT, RL](pretraining_sft_rl.jpeg)\n</div>\n\nDeepSeek-R1-Zero demonstrates that large language models (LLM) can develop strong reasoning skills using only Reinforcement Learning (RL), without needing the traditional step of Supervised Fine-Tuning (SFT). Unlike previous methods that relied heavily on labeled data (showing the AI how to solve problems step by step) to teach models how to reason, DeepSeek-R1-Zero shows that reasoning abilities—like checking its own answers, thinking step-by-step, etc—can emerge naturally through RL alone. This breakthrough proves that models can learn to solve complex problems just by practicing and receiving feedback (correct or not), simplifying the training process and opening potentially more powerful ways to build AI reasoning capabilities.\n\n<div style=\"text-align: center;\">\n![Pure RL VS SFT](RL_vs_SFT.jpg)\n</div>\n\n\nNote: this practice problem will go through this article as an example until the end. \n\n## Question 2: What does the DeepSeek-R1-Zero training process look like?\n\nThe goal of training is to \"steer\" the base LLM (like GPT-4o or DeepSeek V3) towards generating correct and well-formatted answers more consistently. Think of it like upgrading the model – from GPT-4o to a hypothetical \"o3-mini-high,\" or from DeepSeek V3 to DeepSeek-R1-Zero. \n\nThe left side of the below figure is the base model while the right side is the prompted version with GRPO based RL training. As you can see the left one fails at the question while the right one succeeds also with the reasoning process. The figure comes from reference 4.  \n\n<div style=\"text-align: center;\">\n![With VS Without Reasoning](with_vs_without_reasoning.jpg)\n</div>\n\nHere's how it works:\n\nPrompting the Model: We feed a practice problem (like the math problem about Natalia's clips in the below table) to the current version of the LLM, which we'll call the \"old LLM.\"\n\nGenerating Responses: run the old LLM with the practice problem as prompt several times to get answers (technically called \"completions\"). Some of these might be correct, some incorrect, and some might have formatting issues.\n\nScoring the Responses: A \"reward function\" evaluates each response, assigning scores based on:\nCorrectness: How accurate the answer is.\nFormat: Whether the response follows a desired structure (e.g., showing its steps).\n\nUpdating the Model: The crucial step! Using a specialized algorithm called GRPO (Generalized Relative Policy Optimization), the LLM's internal parameters (the \"weights\" that control its behavior) are adjusted. These adjustments are guided by the reward scores: responses with higher scores \"pull\" the model's parameters in their direction, making it more likely to generate similar responses in the future. This updated model is now the \"new LLM.\"\n\nIteration: We set the \"old LLM\" to be the \"new LLM\" (the updated version) and repeat steps 1-4 many, many times. With each iteration, the model gets progressively better at generating correct and well-formatted answers. This iterative process continues until we have a version of the LLM with significantly improved reasoning abilities – in this case, DeepSeek-R1-Zero.\n\nIt's like teaching through trial and error - the model keeps attempting problems, learns from its successes and failures, and gradually becomes better at solving them. The final result is a more capable model like DeepSeek-R1-Zero.\n\nLet’s dive deep into reward functions a bit. The DeepSeek paper mentioned two types of rewards: accuracy and format. An implementation could be but not limited to as below. \n\n\n## Code Example\n\n```python\n# Your code here\ndef correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n    responses = [completion[0]['content'] for completion in completions]\n    q = prompts[0][-1]['content']\n    extracted_responses = [extract_xml_answer(r) for r in responses]\n    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n```\n\n```python\n# Your code here\ndef strict_format_reward_func(completions, **kwargs) -> list[float]:\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.match(pattern, r) for r in responses] \n    return [0.5 if match else 0.0 for match in matches]\n```\n\n## Question 3: understand the Group Relative Policy Optimization (GRPO) algorithm in detail\n\n<div style=\"text-align: center;\">\n![](grpo1.jpg)\n![](grpo2.jpg)\n</div>\n\nThe GRPO algorithm, as described in the paper, begins by sampling a group of outputs {o_1, o_2, ..., o_G} from the old LLM (denoted as π_θold). This is done by feeding the same training example (the prompt, q) to the old LLM multiple times and collecting the different responses it generates.\n\nThe training then focuses on maximizing the objective function (equation (1)). This objective function is, at its core, a complex function of the new LLM's parameters (θ). Standard gradient descent algorithms can be used to efficiently update these parameters and maximize the function's value.\n\nThe key to understanding how GRPO improves the LLM lies in maximizing the objective function (equation (1)). And the heart of that process is within the part highlighted by the \"red rectangle.\"\n\nLet’s zoom into the red rectangle, which contains two parts: (1) probability ratio (2) Advantage function Ai.\n**Advantage function** Ai is defined in the equation (3) and pretty straightforward. A_i tells us whether a particular output (o_i) is better or worse than the average output for a given prompt.\nIf the reward (R_i) for o_i is higher than the average reward, A_i is positive (good).\nIf the reward for o_i is lower than the average reward, A_i is negative (bad).\n \n**Probability ratio:** measures how much the probability of generating the completion o_i for prompt q has changed between the new policy π_\\theta and the old policy π_{\\theta_{old}}.\n- Ratio > 1: The new LLM is more likely to generate o_i than the old LLM.\n- Ratio < 1: The new LLM is less likely to generate o_i than the old LLM.\n\n**The Magic: Multiplying Ratio and Advantage**\n- If A_i > 0 (good completion) and Ratio > 1 (new policy makes it more likely), we want to encourage this change. We want to increase the probability of generating this good completion even more.\n- If A_i > 0 and Ratio < 1 (new policy makes it less likely), we want to discourage this change. We want to increase the probability of generating this good completion.\n- If A_i < 0 (bad completion) and Ratio > 1 (new policy makes it more likely), we want to discourage this change. We want to decrease the probability of generating this bad completion.\n- If A_i < 0 and Ratio < 1 (new policy makes it less likely), we want to encourage this change. We want to decrease the probability of generating this bad completion even further.\n\nBeyond the Red Rectangle: Stability and Generalization\n\nThe \"clip\" and \"KL\" terms in the full objective function play important roles in ensuring the training process remains stable and the model generalizes well:\n\n- Clipping (clip): The clip function acts like a \"brake,\" preventing the probability ratio from changing too drastically in a single update. This avoids large, potentially destabilizing parameter adjustments.\n\n- KL Divergence (KL): The KL divergence term measures how much the new LLM's behavior differs from a \"reference\" policy (often the old LLM). By penalizing large deviations, we encourage the model to improve incrementally and retain some of its desirable characteristics. This helps the model perform well on new, unseen problems and encourages exploration of different solutions.\n\n## Question 4: What makes R1-zero work, is it GRPO or something else?\n\nAs an engineer with no prior LLM/RL experience, I found the GRPO algorithm surprisingly understandable after just a weekend of study. It's essentially a refined version of the established PPO algorithm, so for experts in the field, it's likely not a revolutionary leap. The real challenge, and where DeepSeek-R1-Zero truly shines, is in the engineering implementation, for example how to define the reward functions. This highlights the critical importance of exceptionally talented engineers, perhaps even more so than simply acquiring more computing power.\n\nThe idea of using pure RL to improve a base model isn't new, but previous attempts haven't matched DeepSeek's results. I suspect two key factors are at play: first, a very strong foundation in the base model itself; and second, the quality of the training data (the practice problems). DeepSeek hasn't open-sourced their training data, which is a strong indication of its crucial and proprietary nature.\n\nIn essence, DeepSeek-R1-Zero's success likely boils down to a potent combination of: brilliant engineering, carefully curated training examples, and a robust base model.\n\nThis article is a collaboration with DeepSeek-R1, Gemini 2.0 Flash Thinking Experimental 01-21 (Google really needs to work hard on the naming), ChatGPT o3-mini-high, Claude. Gemini is slightly better than R1, but both of them are better than o3-mini-high in terms of reasoning. Claude is still really good at polishing writing but Gemini 2.0 Pro Experimental 02-05 is at least comparable. The conclusion is there is no need to be a member of Claude anymore. :-). \n\n## Reference\n- 深度强化学习(1/5)：基本概念  Deep Reinforcement Learning (1/5)\n- https://x.com/karpathy/status/1885026028428681698\n- DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, link\n- Train your own R1 reasoning model with Unsloth (GRPO) link\n- https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.24","theme":["cosmo","brand"],"math":"mathjax","anchor-sections":true,"title-block-banner":true,"title":"Explain DeepSeek-R1-Zero to My Little Aurora","author":"Zhong Zhang","date":"2025-09-14","categories":["ML","LLM","RL"],"description":"A brief description of your post"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}